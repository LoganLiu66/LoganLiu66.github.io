<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-128x128.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-128x128.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"loganliu66.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Flow模型 GAN和VAE都没有显式地（explicitly）对真实数据的概率密度函数（probability density function，PDF） $p(\mathbf{x}),\mathbf{x}\in\mathcal{D}$ 进行建模，而是采用对抗训练或优化上界（Evidence Lower Bound，ELBO）的方式避开概率计算，因为 $p(\mathbf{x})$太难算了！以带">
<meta property="og:type" content="article">
<meta property="og:title" content="Flow Model">
<meta property="og:url" content="https://loganliu66.github.io/FlowModel/index.html">
<meta property="og:site_name" content="Logan Liu&#39;s Blog">
<meta property="og:description" content="Flow模型 GAN和VAE都没有显式地（explicitly）对真实数据的概率密度函数（probability density function，PDF） $p(\mathbf{x}),\mathbf{x}\in\mathcal{D}$ 进行建模，而是采用对抗训练或优化上界（Evidence Lower Bound，ELBO）的方式避开概率计算，因为 $p(\mathbf{x})$太难算了！以带">
<meta property="og:locale">
<meta property="og:image" content="https://loganliu66.github.io/images/image-20231128213643107.png">
<meta property="og:image" content="https://loganliu66.github.io/images/image-20231128213808501.png">
<meta property="og:image" content="https://loganliu66.github.io/images/image-20231128214052260.png">
<meta property="og:image" content="https://loganliu66.github.io/images/image-20231128214107561.png">
<meta property="og:image" content="https://loganliu66.github.io/images/image-20231128214127526.png">
<meta property="article:published_time" content="2023-11-28T08:47:50.000Z">
<meta property="article:modified_time" content="2023-12-03T06:01:07.406Z">
<meta property="article:author" content="Logan Liu">
<meta property="article:tag" content="ASR">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://loganliu66.github.io/images/image-20231128213643107.png">

<link rel="canonical" href="https://loganliu66.github.io/FlowModel/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Flow Model | Logan Liu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Logan Liu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-others">

    <a href="/others/" rel="section"><i class="fa fa-sitemap fa-fw"></i>others</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://loganliu66.github.io/FlowModel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon-128x128.png">
      <meta itemprop="name" content="Logan Liu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Logan Liu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Flow Model
        </h1>

        <div class="post-meta">

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1>Flow模型</h1>
<p>GAN和<a target="_blank" rel="noopener" href="https://www.notion.so/GAN-VAE-Flow-Diffusion-AR-e4ed1e1c92f14dd18f48ddb2de560205?pvs=21">VAE</a>都没有显式地（explicitly）对真实数据的概率密度函数（probability density function，PDF） $p(\mathbf{x}),\mathbf{x}\in\mathcal{D}$ 进行建模，而是采用对抗训练或优化上界（Evidence Lower Bound，ELBO）的方式避开概率计算，因为 $p(\mathbf{x})$太难算了！以带隐变量的生成模型 $p(\mathbf{x})=\int p(\mathbf{x}\mid\mathbf{z})p(\mathbf{z})d\mathbf{z}$ 为例，因为不可能遍历所有的潜在编码 $\mathbf{z}$，所以 $p(\mathbf{x})$的计算是非常困难的（intractable）。</p>
<p>Flow-based生成模型通过<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.05770">normalizing flows</a>解决了这个问题，normalizing flows是一个非常强大的概率密度函数计算工具。计算出 $p(\mathbf{x})$之后我们可以有效地完成下游任务，如：采样出不存在于训练集但真实存在的样本点（data generation）、计算某个时间可能发生的概率（density estimation）等等。</p>
<span id="more"></span>
<h2 id="线性代数基础概述">线性代数基础概述</h2>
<p>在flow-based模型之前，先来了解线性代数中两个重要的概念：雅可比行列式（Jacobian Determinant）与变量变换定理/换元积分法（the change of variable rule）；前者阐述了雅可比行列式的定义，后者阐述了如何用雅可比行列式进行变量变换；</p>
<h3 id="雅可比矩阵和行列式">雅可比矩阵和行列式</h3>
<p>如果一个函数 $\mathbf{f}$把一个n维的输入映射为m维的输出， $\mathbf{f}: \mathbb{R}^n\mapsto\mathbb{R}^m$，那么这个函数的一阶（first-order）偏导数（partial derivative）叫做雅可比矩阵（Jacobian Matrix），表示为 $\mathbf{J}$。 $\mathbf{J}$的第i行、第j列的计算公式为 $\mathbf{J_{ij}=\frac{\partial{f_i}}{\partial{x_j}}}$</p>
<p>$$<br>
\mathbf{J}=\left[\begin{array}{ccc}\frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}\end{array}\right]<br>
$$</p>
<p>雅可比行列式只存在于方阵（square matrices）中，是一个由矩阵中所有元素参与计算得到的实数；行列式的绝对值表示在该数据点处对空间扩展和收缩的程度（how much expands or contracts space），也可以理解为行列式中的行或列向量所构成的超平行多面体的<a target="_blank" rel="noopener" href="https://www.cnblogs.com/AndyJee/p/3491487.html">有向面积或有向体积</a>。对于二阶行列式：</p>
<p>$$<br>
\left[\begin{array}{cc} a_1 &amp; a_2  \\ b_1 &amp;b_2\end{array} \right]=a_1b_2-a_2b_1<br>
$$</p>
<p>例如向量 $a={1,0}, b={0,1}$所构成的有向面积为1<em>1-0</em>0=1。而对于雅可比矩阵，对应函数 $\mathbf{f}$在点 $\mathbf{x}$的微分，矩阵的每一项代表各个维度下面积或体积的伸缩因子。</p>
<p>将上面二阶行列式的计算扩展到n阶有如下公式：</p>
<p>$$<br>
\operatorname{det} M=\operatorname{det}\left[\begin{array}{cccc}a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1 n} \\ a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2 n} \\ \vdots &amp; \vdots &amp; &amp; \vdots \\ a_{n 1} &amp; a_{n 2} &amp; \ldots &amp; a_{n n}\end{array}\right]=\sum_{j_1 j 2 \ldots j_n}(-1)^{\tau\left(j_1 j_2 \ldots j_n\right)} a_{1 j_1} a_{2 j_2} \ldots a_{n j_n}<br>
$$</p>
<p>其中 $j_1j_2…j_n$是 $1,2,…,n$的一个排列， $\sum\limits_{j_1 j_2 \ldots j_n}$是遍历所有 $1,2,…,n$排列 ${j_1 j_2 \ldots j_n}$求和；方阵 $M$的行列式有如下性质：</p>
<ul>
<li>如果 $\operatorname{det}(M)=0$，则矩阵 $M$不可逆，否则矩阵可逆；</li>
<li>行列式的乘积等于矩阵乘积的行列式 $\operatorname{det}(AB)=\operatorname{det}(A)\operatorname{det}(B)$；（<a target="_blank" rel="noopener" href="https://proofwiki.org/wiki/Determinant_of_Matrix_Product">证明</a>）</li>
</ul>
<h3 id="变量变换定理-Change-of-Variable-Theorem">变量变换定理(Change of Variable Theorem)</h3>
<p>给定一维随机变量$z$以及概率密度函数 $z\sim\pi(z)$,我们使用映射函数构建一个新的随机变量 $x=f(z)$,函数 $f$是可逆的，所以有 $z=f^{-1}(x)$, 如何得到新的随机变量 $x$的概率密度函数 $p(x)$呢? 由概率分布的定义可知：</p>
<p>$$<br>
\int p(x)dx = \int \pi(z)dz = 1 \<br>
\therefore  p(x)=\pi(z)\left|\frac{dz}{dx}\right|=\pi(f^{-1}(x))\left|\frac{df^{-1}}{dx}\right|=\pi(f^{-1}(x))\left|(f^{-1})'(x)\right|<br>
$$</p>
<p>根据定义， $\int\pi(z)dz$是无限（infinite）多个宽度无穷小（infinitesimal）的矩形面积之和，在位置 $z$处的矩形的高度就是其密度函数 $\pi(z)$。由 $z=f^{-1}(x)$可得 $\frac{\Delta z}{\Delta x}=(f^{-1}(x))‘$,  $\Delta z=(f^{-1}(x))’ \Delta x$, 所以此处的 $(f^{-1}(x))'$ 可以任务是变量 $x$和 $z$在两个不同的坐标系中对应位置处的矩形的面积的比值。</p>
<p>多维变量的变换与一维变量的变换比较相似：</p>
<p>$$<br>
\begin{aligned}\mathbf{z} &amp; \sim \pi(\mathbf{z}), \mathbf{x}=f(\mathbf{z}), \mathbf{z}=f^{-1}(\mathbf{x}) \\ p(\mathbf{x}) &amp; =\pi(\mathbf{z})\left|\operatorname{det} \frac{d \mathbf{z}}{d \mathbf{x}}\right|=\pi\left(f^{-1}(\mathbf{x})\right)\left|\operatorname{det} \frac{d f^{-1}}{d \mathbf{x}}\right|\end{aligned}<br>
$$</p>
<p>其中， $\operatorname{det} \frac{d f^{-1}}{d \mathbf{x}}$是函数 $f$的雅可比行列式。</p>
<h3 id="实例">实例</h3>
<p><a target="_blank" rel="noopener" href="https://www.gwylab.com/note-flow_based_model.html">https://www.gwylab.com/note-flow_based_model.html</a></p>
<h2 id="Normalizing-Flows">Normalizing Flows</h2>
<blockquote>
<p>数学原理影响我们对模型的选择</p>
</blockquote>
<p>密度估计（density estimation）在很多机器学习问题上都有应用，但一个好的密度估计又很难做到。例如，由于深度学习中需要进行反向传播，所以我们希望后验概率分布（posterior， $p(z|x)$）尽可能的简单，以方便求导。这也是为什么隐变量生成模型（latent variable generative model），如VAE，中通常使用高斯分布，尽管现实世界中分布往往比高斯分布更复杂。</p>
<p>Normalizing Flow模型可以很好的进行分布估计（distribution approximation），normalizing flow可以通过一系列的可逆变换函数将一个简单的分布变成复杂的分布。利用变量变化定理，可以反复的替换其中一部分变量，直到得到一个新的概率分布。</p>
<div>
<img src="../images/image-20231128213643107.png" width="100%"/>
<center>图1. Normalizing Flow模型变换示意图，将一个简单的分布 $p_0(z_0)$逐步替换成一个复杂的分布 $p_K(z_K)$</center>
</div>
<center><figure src="../images/image-20231128213643107.png" width="100%" title="图1.Normalizing Flow模型将一个简单的分布 $p_0(\mathbf z_0)$逐步替换成一个复杂的分布 $p_K(\mathbf z_K)$"></center>
<p>图1. Normalizing Flow模型变换示意图，将一个简单的分布 $p_0(z_0)$逐步替换成一个复杂的分布 $p_K(z_K)$</p>
<p>根据图1我们有</p>
<p>$$<br>
\begin{aligned}\mathbf z_{i-1}&amp;\sim p_{i-1}(\mathbf z_{i-1}) \\ \mathbf z_{i} &amp;= f_{i}(\mathbf z_{i-1}), thus \\ \mathbf z_{i-1} &amp;= f^{-1}_{i}(\mathbf z_{i}) \end{aligned}<br>
$$</p>
<p>根据多维变量的变换定律有：</p>
<p>$$<br>
\begin{aligned} p_i(\mathbf z_i)&amp;=p_{i-1}(\mathbf z_{i-1})\left|\operatorname{det}\frac{d \mathbf z_{i-1}}{d \mathbf z_{i}}\right| \\ &amp;=p_{i-1}(\mathbf z_{i-1})\left|\left(\operatorname{det}\frac{d \mathbf z_{i}}{d \mathbf z_{i-1}}\right)^{-1}\right| \\ &amp;=p_{i-1}(\mathbf z_{i-1})\left|\left(\operatorname{det}\frac{d f_{i}(\mathbf z_{i-1})}{d \mathbf z_{i-1}}\right)^{-1}\right|;反函数定理 \\ &amp;=p_{i-1}(\mathbf z_{i-1})\left|\operatorname{det}\frac{d f_{i}(\mathbf z_{i-1})}{d \mathbf z_{i-1}}\right|^{-1};雅可比行列式的反函数性质 \end{aligned}<br>
$$</p>
<p>所以有：</p>
<p>$$<br>
log(p_i(\mathbf z_i))=log(p_{i-1}(\mathbf z_{i-1})) - log\left|\operatorname{det}\frac{d f_{i}(\mathbf z_{i-1})}{d \mathbf z_{i-1}}\right|<br>
$$</p>
<p>给定这样一连串的概率密度函数，我们知道每两个连续的变量之间的关系，从而可以知道初始分布 $z_0$与输出 $x$之间的转换公式：</p>
<p>$$<br>
\begin{aligned}\mathbf{x}=\mathbf{z}_K &amp; =f_K \circ f_{K-1} \circ \cdots \circ f_1\left(\mathbf{z}_0\right) \\ \log p(\mathbf{x})=\log \pi_K\left(\mathbf{z}_K\right) &amp; =\log \pi_{K-1}\left(\mathbf{z}_{K-1}\right)-\log \left|\operatorname{det} \frac{d f_K}{d \mathbf{z}_{K-1}}\right| \\ &amp; =\log \pi_{K-2}\left(\mathbf{z}_{K-2}\right)-\log \left|\operatorname{det} \frac{d f_{K-1}}{d \mathbf{z}_{K-2}}\right|-\log \left|\operatorname{det} \frac{d f_K}{d \mathbf{z}_{K-1}}\right| \\ &amp; =\ldots \\ &amp; =\log \pi_0\left(\mathbf{z}_0\right)-\sum_{i=1}^K \log \left|\operatorname{det} \frac{d f_i}{d \mathbf{z}_{i-1}}\right|\end{aligned}<br>
$$</p>
<p>随机变量经过的路径 $z_i=f_i(z_{i-1})$就是flow，连续分布 $\pi_i$形成的完整链路就是normalizing flow。上述公式成立，需要变换函数 $f_i$ 满足如下条件：</p>
<ul>
<li>可逆</li>
<li>雅可比行列式容易计算</li>
</ul>
<h2 id="工程实现">工程实现</h2>
<p>有了normalizing flow的数学原理，我们看在工程上如何实现它，尤其是神经网络应用在何处。</p>
<h3 id="RealNVP">RealNVP</h3>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/#realnvp">RealNVP</a>通过叠加一系列可逆的对射变换函数（bijective transformation function）实现normalizing flow。每个对射函数 $f:x\mapsto y$,被称为仿射耦合层（affine coupling layer），它把输入的维度分成两部分：</p>
<ul>
<li>前 $d$ 维保持不变；</li>
<li>第 $d+1$至 $D$维，执行仿射变换（“scale-and-shift”），放缩(scale)和移动(shift)的参数都是以前 $d$ 维作为输入的函数；</li>
</ul>
<p>$$<br>
\begin{array}{cc} \mathbf y_{1:d}=\mathbf x_{1:d} \\ <br>
\mathbf y_{d+1:D}=\mathbf x_{d+1:D}\odot \operatorname{exp}(s(\mathbf x_{1:d})) + t(\mathbf x_{1:d}) \end{array}<br>
$$</p>
<p>其中 $s(\cdot)$和 $t(\cdot)$是scale和translation函数，这两个函数将 $\mathbb{R^d}$映射为 $\mathbb{R^{D-d}}$, $\odot$是逐元素相乘。下面让我们验证这种变换是否满足上述两个条件；</p>
<p><strong>条件1:</strong> 可逆</p>
<p>可逆比较容易证明，如下：</p>
<p>$$<br>
\begin{cases}\begin{array}{ll}  \mathbf { y } _ { 1 : d }  = \mathbf { x } _ { 1 : d }  \\  \mathbf { y } _ { d + 1 : D }  = \mathbf { x } _ { d + 1 : D } \odot \operatorname { e x p } ( s ( \mathbf { x } _ { 1 : d } ) ) + t ( \mathbf { x } _ { 1 : d } ) \end{array} \end{cases} \Leftrightarrow \begin{cases}\begin{array}{ll}  \mathbf { x } _ { 1 : d }    = \mathbf { y } _ { 1 : d }  \\  \mathbf { x } _ { d + 1 : D }   = (\mathbf { y } _ { d + 1 : D }  - t ( \mathbf { y } _ { 1 : d } )) \odot \operatorname { e x p } ( -s ( \mathbf { y } _ { 1 : d } ) )\end{array} \end{cases}<br>
$$</p>
<p><strong>条件2:</strong> 雅可比行列式比较容易计算</p>
<p>上述变换函数的雅可比行列式如下所示，是一个下三角矩阵：</p>
<p>$$<br>
\mathbf{J} = \left[\begin{array}{cc}\mathbb{I} &amp; \mathbb{0}_{d\times(D-d)} \\ <br>
\frac{\partial \mathbf{y}_{d+1: D}}{\partial \mathbf{x}_{1: d}} &amp; \operatorname{diag}(\operatorname{exp}(s(\mathbf x_{1:d})))\end{array}\right]<br>
$$</p>
<p>下三角矩阵的行列式为（三角阵的行列式等于对角线元素之积）：</p>
<p>$$<br>
\operatorname{det}(\mathbf{J})=\prod_{j=1}^{D-d} \exp \left(s\left(\mathbf{x}_{1: d}\right)\right)_j=\exp \left(\sum_{j=1}^{D-d} s\left(\mathbf{x}_{1: d}\right)_j\right)<br>
$$</p>
<p>以上证明了RealNVP满足可逆和雅可比行列式容易计算两个条件。此外，由条件1的证明可知计算 $f^{-1}$不需要计算 $s$和 $t$的逆函数，雅可比行列式的计算也不需要计算 $s$和 $t$的行列式，所以 $s$和 $t$可以是任意复杂度，比如 $s$和 $t$均可以是一个神经网络。</p>
<p>在一个仿射耦合层（affine coupling layer）中，某些纬度是保持不变的，这会导致生成数据中总有一片区域看起来像固定的图样。为了保证所有维度都有被替换的机会，会不断的交换复制模块（copy）与仿射模块（affine）的顺序，这样最终的生成图像就不会包含完全copy自初始图像的部分。</p>
<h3 id="NICE">NICE</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1410.8516">NICE</a>（Non-linear Independent Component Estimation）是<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/#realnvp">RealNVP</a>的先导工作，与RealNVP不同的是NICE是不包含缩放（scale）项的仿射耦合层，也称加性（additive）耦合层。</p>
<p>$$<br>
\begin{cases}\begin{array} { l l } { \mathbf { y } _ { 1 : d } } { = \mathbf { x } _ { 1 : d } } \\ { \mathbf { y } _ { d + 1 : D } } { = \mathbf { x } _ { d + 1 : D } + m ( \mathbf { x } _ { 1 : d } ) }\end{array} \end{cases} \Leftrightarrow  \begin{cases} \begin{array}{ll}\mathbf{x}_{1: d} =\mathbf{y}_{1: d} \\ \mathbf{x}_{d+1: D} =\mathbf{y}_{d+1: D}-m\left(\mathbf{y}_{1: d}\right)\end{array} \end{cases}<br>
$$</p>
<h3 id="Glow">Glow</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03039">Glow</a>模型扩展之前的可逆生成模型，如NICE、RealNVP，这些模型在像素维度上进行划分，执行仿射对偶变换。而Glow在通道（channel）维度上进行copy和affine，在每次耦合变换中按顺序选择一个进行copy，其他的通道进行affine。为了让每个通道都有被替换的机会，引入W矩阵，帮我们决定按什么样的顺序进行copy和affine，这种方法叫做1x1 convolution。1x1 convolution决定在每次仿射变换前对图片哪些区域进行像素对调，而copy和affine的顺序保持不变，这样和对调copy和affine的效果是一致的。</p>
<div>
<img src="../images/image-20231128213808501.png" width="30%"/>
<center>图2. 1x1 convolution</center>
</div>
<p>Glow中的一个flow包含如下子步骤：</p>
<div>
<img src="../images/image-20231128214052260.png" width="30%"/>
<center>图3. One step of flow in the Glow model. (Image source: [Kingma and Dhariwal, 2018](https://arxiv.org/abs/1807.03039))</center>
</div>
<p><strong>步骤1: Activation Normalization（actnorm）</strong></p>
<p>类似于batch normalization，通过scale和bias参数进行仿射变换（和仿射耦合层不同的是不包含copy部分，而是对所有维度进行仿射变换），这些参数是可训练的。</p>
<p><strong>步骤2: invertible 1x1 conv</strong></p>
<div>
<img src="../images/image-20231128214107561.png" width="30%"/>
<center>图4. invertible 1x1 convolution</center>
</div>
<p>在每两个Glow flow之间，应用1x1卷积（卷积核的大小等于输入输出的channel数）可以产生channel顺序的任意排列，这样每个通道都有被替换的机会。</p>
<p>对于形状为 $h\times w \times c$ 的输入tensor $\mathbf{h}$，应用形状为  $c \times c$ 的可学习的1x1卷积权重矩阵 $\mathbf{W}$，得到的输出tensor为 $h\times w \times c$，用函数表示为 $f=\operatorname{conv2d}(\mathbf{h};\mathbf{W})$ 。为了应用变量变换规则，我们需要计算行列式 $|\operatorname{det}df/d\mathbf{h}|$的值，而通过推导可知可以直接通过计算矩阵 $\mathbf{W}$ 的行列式得到 $|\operatorname{det}df/d\mathbf{h}|$的值。计算过程如下：</p>
<p>1x1卷积的输入输出可以看做是 $h\times w$ 的矩阵， $\mathbf{x}_{ij} \in \mathbb{R}^c(i=1,\ldots,h,j=1,\ldots,w)$是矩阵中的一个数据点，和权重矩阵 $\mathbf{W}$ 相乘后得到对应的输出  $\mathbf{y}_{ij}\in \mathbb{R}^c(i=1,\ldots,h,j=1,\ldots,w)$,每个数据点的导数 $d\mathbf{x}_{ij}\mathbf{W}/d\mathbf{x}_{ij}=\mathbf{W}$,  $h\times w$ 个数据点的行列式为：<br>
$$<br>
\operatorname{log}\left|\operatorname{det}\frac{\partial{\operatorname{conv2d}(\mathbf{h};\mathbf{W})}}{\partial\mathbf{h}} \right|=\operatorname{log}\left|\operatorname{det}\mathbf{W}^{h\times w}\right|={h \cdot w} \cdot \operatorname{log}\left|\operatorname{det}\mathbf{W}\right|<br>
$$</p>
<p>因为 $\mathbf{W}$ 维度较小，所以 $\mathbf{W}$ 的行列式比较容易计算，1x1卷积的逆变换 $\mathbf{W}^{-1}$也比较容易计算，所以整体的计算量是可控的；</p>
<p><strong>步骤3: 仿射对偶变换</strong></p>
<p>这里的仿射对偶变换和RealNVP中的是一致的。</p>
<p>所以Glow的各个子步骤对应的逆函数和行列式计算如下所示：</p>
<div>
<img src="../images/image-20231128214127526.png" width="100%"/>
<center>图5. 各个子步骤对应的逆函数和行列式计算</center>
</div>
<h2 id="疑问">疑问</h2>
<ul>
<li>为什么flow模型要求变换函数是可逆的？</li>
</ul>
<blockquote>
<p>这是normalizing flow进行密度估计的函数成立的条件；</p>
</blockquote>
<h2 id="参考：">参考：</h2>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/uploads/prod/2022/12/Generative-Models-for-TTS.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2022/12/Generative-Models-for-TTS.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://www.gwylab.com/note-flow_based_model.html">https://www.gwylab.com/note-flow_based_model.html</a></p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">https://lilianweng.github.io/posts/2018-10-13-flow-models/</a></p>
<p><a target="_blank" rel="noopener" href="https://slyne.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020/04/01/DGM-flow/#">https://slyne.github.io/深度学习/2020/04/01/DGM-flow/#</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/AndyJee/p/3491487.html">https://www.cnblogs.com/AndyJee/p/3491487.html</a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/AndyJee/p/3491487.html">https://www.cnblogs.com/AndyJee/p/3491487.html</a>)</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ASR/" rel="tag"># ASR</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/VALL-E/" rel="prev" title="VALL-E">
      <i class="fa fa-chevron-left"></i> VALL-E
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">Flow模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0"><span class="nav-number">1.1.</span> <span class="nav-text">线性代数基础概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5%E5%92%8C%E8%A1%8C%E5%88%97%E5%BC%8F"><span class="nav-number">1.1.1.</span> <span class="nav-text">雅可比矩阵和行列式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E5%8F%98%E6%8D%A2%E5%AE%9A%E7%90%86-Change-of-Variable-Theorem"><span class="nav-number">1.1.2.</span> <span class="nav-text">变量变换定理(Change of Variable Theorem)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">1.1.3.</span> <span class="nav-text">实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Normalizing-Flows"><span class="nav-number">1.2.</span> <span class="nav-text">Normalizing Flows</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.</span> <span class="nav-text">工程实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RealNVP"><span class="nav-number">1.3.1.</span> <span class="nav-text">RealNVP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NICE"><span class="nav-number">1.3.2.</span> <span class="nav-text">NICE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Glow"><span class="nav-number">1.3.3.</span> <span class="nav-text">Glow</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%96%91%E9%97%AE"><span class="nav-number">1.4.</span> <span class="nav-text">疑问</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%EF%BC%9A"><span class="nav-number">1.5.</span> <span class="nav-text">参考：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Logan Liu"
      src="/images/favicon-128x128.png">
  <p class="site-author-name" itemprop="name">Logan Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Logan Liu</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
