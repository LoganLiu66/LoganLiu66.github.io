<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-128x128.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-128x128.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"loganliu66.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="摘要 之前的非自回归模型在训练时往往需要额外的对齐知识（如fastspeech先蒸馏出一个duration predictor，fastspeech2使用MFA工具），而在本文中使用Flow和动态规划搜索最可能的文本与语音表示之间的单调对齐(monotonic alignment)路径。最终的结果表明这种强制单调硬对齐(enforcing hard)可以使TTS更加鲁棒，而Flow的使用可以使合成">
<meta property="og:type" content="article">
<meta property="og:title" content="Glow-TTS">
<meta property="og:url" content="https://loganliu66.github.io/GlowTTS/index.html">
<meta property="og:site_name" content="Logan Liu&#39;s Blog">
<meta property="og:description" content="摘要 之前的非自回归模型在训练时往往需要额外的对齐知识（如fastspeech先蒸馏出一个duration predictor，fastspeech2使用MFA工具），而在本文中使用Flow和动态规划搜索最可能的文本与语音表示之间的单调对齐(monotonic alignment)路径。最终的结果表明这种强制单调硬对齐(enforcing hard)可以使TTS更加鲁棒，而Flow的使用可以使合成">
<meta property="og:locale">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS1.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS2.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS3.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS4.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS5.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS6.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS7.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS8.png">
<meta property="og:image" content="https://loganliu66.github.io/images/GlowTTS9.png">
<meta property="article:published_time" content="2023-12-16T08:47:50.000Z">
<meta property="article:modified_time" content="2023-12-22T02:03:11.160Z">
<meta property="article:author" content="Logan Liu">
<meta property="article:tag" content="TTS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://loganliu66.github.io/images/GlowTTS.png">

<link rel="canonical" href="https://loganliu66.github.io/GlowTTS/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Glow-TTS | Logan Liu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Logan Liu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-others">

    <a href="/others/" rel="section"><i class="fa fa-sitemap fa-fw"></i>others</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://loganliu66.github.io/GlowTTS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon-128x128.png">
      <meta itemprop="name" content="Logan Liu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Logan Liu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Glow-TTS
        </h1>

        <div class="post-meta">

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="摘要">摘要</h2>
<p>之前的非自回归模型在训练时往往需要额外的对齐知识（如fastspeech先蒸馏出一个duration predictor，fastspeech2使用MFA工具），而在本文中使用<strong>Flow和动态规划</strong>搜索最可能的文本与语音表示之间的<strong>单调对齐</strong>(monotonic alignment)路径。最终的结果表明这种强制单调硬对齐(enforcing hard)可以使TTS更加鲁棒，而Flow的使用可以使合成的语音更快、多样(diverse)以及可控(controllable)。此外，在合成质量差不多的情况下，Glow-TTS的生成速度比自回归的Tacotron2快了一个数量级。</p>
<span id="more"></span>
<h2 id="前置知识">前置知识</h2>
<ul>
<li>非自回归(Non AutoRegressive, NAR)TTS与自回归(AutoRegressive, AR)TTS的区别在于AR TTS在合成语音时需要step by step，合成速度较慢。而AR TTS可以parallel生成。</li>
<li><a target="_blank" rel="noopener" href="https://www.notion.so/Flow-d0415a7ad07040f19c3ff7a2c3023e35?pvs=21">Flow Model</a>：可以将一个简单的分布变成一个复杂的分布。</li>
<li>FastSpeech先蒸馏出一个duration predictor得到文本与语音帧的对齐关系，FastSpeech2使用MFA工具得到文本与语音帧的对齐关系。本文通过单调对齐搜索(MAS)找到对齐关系。</li>
<li><strong>WaveGlow、FloWaveNet</strong>都是早于该项工作将Flow用于TTS，但是是用于vocoder建模；而本文用于acoustic model。在此之前也有<strong>Flowtron、Flow-TTS</strong>等工作将Flow用于acoustic model建模，但是都是进行soft attention alignment，与本文的hard monotonic alignment有所不同。</li>
</ul>
<h2 id="模型">模型</h2>
<div  style="text-align: center;">
<img src="../images/GlowTTS.png" width="70%"/>
<center>Glow-TTS训练与推理流程</center>
</div>
<h3 id="训练与推理过程">训练与推理过程</h3>
<p><strong>训练过程</strong></p>
<p>Glow-TTS通过flow-based decoder $f_{dec}:z\rightarrow x$ 转换先验条件分布$P_Z(z|c)$ 从而完成对梅尔谱的条件分布(conditional distribution) $P_X(x|c)$ 的建模，通过<a target="_blank" rel="noopener" href="https://www.notion.so/Flow-d0415a7ad07040f19c3ff7a2c3023e35?pvs=21">换元法</a>，我们可以精确计算$P_X(x|c)$ 的似然函数：</p>
<p>$$<br>
\operatorname{log}P_X(x|c)=\operatorname{log}P_Z(z|c)+\operatorname{log} \left|\operatorname{dec} \frac{\partial f^{-1}_{dec}(x)}{\partial x}\right|<br>
$$</p>
<p>我们可以用对齐函数(alignment function) $A$ 以及神经网络 (text encoder) 参数 $\theta$ 参数化先验分布$P_Z(z|c)=P_Z(z|c;\theta,A)$ 。text encoder将 text condition $c=c_{1:T_{text}}$ 映射为多元高斯分布的统计量 $\mu=\mu_{1:T_{text}}$ 和 $\sigma=\sigma_{1:T_{text}}$，其中 $T_{text}$ 表示文本输入的长度。而对齐函数 $A$ 将多元高斯分布的统计量映射为语音的潜在表示 $z$ 。我们假设对齐函数是单调和<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%BB%A1%E5%B0%84/9274829">满射</a>(surjective)的，以保证不会跳过或者重复文本输入。所以先验分布 $P_Z(z|c)$可以用如下表示：</p>
<p>$$<br>
\operatorname{log} P_Z(z|c;\theta,A) = \sum^{T_{mel}}_{j=1}\operatorname{log} \mathcal{N}(z_j;\mu_{A(j)},\sigma_{A(j)})<br>
$$</p>
<p>$T_{mel}$ 表示输入的梅尔谱的长度。</p>
<p>这样我们的目标就转换成求对齐 $A$ 以及参数 $\theta$ 以最大化数据的似然函数，如下式：</p>
<p>$$<br>
\begin{aligned} \operatorname{max}_{\theta,A} L(\theta, A) &amp;= \operatorname{max}_{\theta,A} \operatorname{log} P_X (x|c; A, \theta) \\ &amp;=\operatorname{max}_{\theta,A}(\sum^{T_{mel}}_{j=1}\operatorname{log} \mathcal{N}(z_j;\mu_{A(j)},\sigma_{A(j)})+\operatorname{log} \left|\operatorname{dec} \frac{\partial f^{-1}_{dec}(x)}{\partial x}\right|) \end{aligned}<br>
$$</p>
<p>但是该公式的求解是intractable的（类似<a target="_blank" rel="noopener" href="https://www.notion.so/GAN-VAE-Flow-Diffusion-AR-e4ed1e1c92f14dd18f48ddb2de560205?pvs=21">VAE</a>），为了克服intractability的问题，我们降低参数的搜索空间，并将对齐问题分解为两个子步骤：(i) 用现在的参数 $\theta$ 以及下式搜索出最可能的单调对齐路径 $A^*$，单调对齐搜索将在下个小节进行介绍 (ii) 更新参数 $\theta$ 以最大化对数似然 $P_X(x|c; A^*,\theta)$ 。</p>
<p>$$<br>
A^* = \operatorname{arg}\operatorname{max}_{A} \operatorname{log} P_X (x|c; A, \theta) = \operatorname{arg} \operatorname{max}_{A}\sum^{T_{mel}}_{j=1}\operatorname{log} \mathcal{N}(z_j\mu_{A(j)},\sigma_{A(j)})<br>
$$</p>
<p>虽然修改后的目标函数不能保证优化到 $\operatorname{max} _{\theta,A} L(\theta, A)$ 的全局最优，但仍提供了一个好的下界（good lower bound）。</p>
<p><strong>推理过程</strong></p>
<p>为了在推理时也能得到最可能的单调对齐路径 $A^*$ ，我们也训练了一个时长预测器(duration predictor) $f_{dur}$去拟合从 $A^*$ 中得到的时长标签，从$A^*$ 中得到的时长标签的公式如下式：</p>
<p>$$<br>
d_i = \sum^{T_{mel}}_{j=1}1_{A^*(j)=i}, i = 1,\ldots,T_{text}<br>
$$</p>
<p>duration predictor的训练过程使用MSE loss计算损失：</p>
<p>$$<br>
L_{dur} = MSE(f_{dur}(sg[f_{enc}(c)]),d)<br>
$$</p>
<p>推理过程中分别使用text encoder和duration predictor得到先验分布的统计量 $\mu$ 和 $\sigma$ 以及对齐 $A$，然后从先验分布中采样出一个隐变量，再通过flow-based decoder合成梅尔谱。</p>
<h3 id="单调对齐搜索">单调对齐搜索</h3>
<p>MAS用于搜索输入语音（实际是从梅尔谱 $x$ 得到的隐变量 $z$ ）和文本（实际是从text encoder得到的先验分布统计量 $\mu$ 和 $\sigma$ ）之间的对齐。</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS1.png" width="70%"/>
</div>
<p>对齐 $A^*$ 是通过递归的方式求得的，伪代码如下图所示，其中 $Q_{i,j}$ 表示 $\mu$ 和 $\sigma$ 的前 $i$ 个元素和 $z$ 的前 $j$ 个元素的最大对数似然（maximum log-likehood），因为对齐符合单调性(monotonic)和满射性(surjection)，所以 $Q_{i,j}$ 可以以递归的方式从 $Q_{i-1,j-1}$和 $Q_{i,j-1}$ 获得，从而大大节省了搜索对齐的时间。</p>
<p>$$<br>
\begin{aligned}Q_{i,j}=\operatorname{max}_{A}\sum_{k=1}^j \operatorname{log} \mathcal{N}(z_k;\mu_{A(k)},\sigma_{A(k)})\\ =\operatorname{max}(Q_{i-1,j-1},Q_{i,j-1}) + \operatorname{log} \mathcal{N}(z_j;\mu_{i},\sigma_{i})\end{aligned}<br>
$$</p>
<p>找到最终的 $Q_{T_{text},T_{mel}}$ 之后，可以通过回溯找到对齐路径；</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS2.png" width="70%"/>
</div>
<h3 id="模型架构">模型架构</h3>
<p>Decoder</p>
<p>flow-based decoder是Glow-TTS的核心。训练阶段我们需要将梅尔谱转换为潜在表示以便进行最大似然估计和对齐搜索。而在推理阶段，我们需要将先验分布变为梅尔谱的分布以便进行高效的并行解码。而flow-based模型符合我们不同阶段都可以并行的需求。具体地，我们的block由多个块(block)组成，每个block都是由activation normalization layer, invertible 1x1 convolution layer, and affine coupling layer组成，affine coupling layer和<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.00002">WaveGlow</a>中的相同。</p>
<p>为了<strong>计算更加高效</strong>，在flow之前将80维的梅尔谱组合成160维的特征图(feature map),如下图(b)，其中前80维是serve的，后80维是要进行scale和translate的。在进行卷积核大小为4的1x1卷积之前，会将160维的特征图分为40组(160=4*40)。为了<strong>让每个group都有affine coupling layer的”可变”(scale and translated)与”不变”(serve)两部分</strong>，以保证能由小的 $\operatorname{det}(M)$ 得到整个flow模型的 $\operatorname{det}(M)$ 的正确性（见<a target="_blank" rel="noopener" href="https://www.notion.so/Flow-d0415a7ad07040f19c3ff7a2c3023e35?pvs=21">推导</a>），所以会进行Channel Mixing（图c的左边）。</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS3.png" width="70%"/>
</div>
<p>invertible 1x1 convolution包含三个子步骤，如上图c所示：</p>
<ol>
<li>Allow Channel Mixing In Each Group（图c的左边）</li>
<li>1x1 convolution （图c的中间）</li>
<li>Reverse Channel Mixing（图c的右边）</li>
</ol>
<p>其对应的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To Allow Channel Mixing In Each Group（图c的左边，Two partitions used for </span></span><br><span class="line"><span class="comment"># coupling layers are colored blue and white, respectively）</span></span><br><span class="line">x = x.view(b, <span class="number">2</span>, c // self.n_split, self.n_split // <span class="number">2</span>, t)</span><br><span class="line">x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>).contiguous().view(b, self.n_split, c // self.n_split, t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1x1 convolution （图c的中间）</span></span><br><span class="line"><span class="keyword">if</span> reverse:</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;weight_inv&quot;</span>):</span><br><span class="line">    weight = self.weight_inv</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    weight = torch.inverse(self.weight.<span class="built_in">float</span>()).to(dtype=self.weight.dtype)</span><br><span class="line">  logdet = <span class="literal">None</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  weight = self.weight</span><br><span class="line">  <span class="keyword">if</span> self.no_jacobian:</span><br><span class="line">    logdet = <span class="number">0</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    logdet = torch.logdet(self.weight) * (c / self.n_split) * x_len <span class="comment"># [b]</span></span><br><span class="line"></span><br><span class="line">weight = weight.view(self.n_split, self.n_split, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">z = F.conv2d(x, weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reverse Channel Mixing（图c的右边）</span></span><br><span class="line">z = z.view(b, <span class="number">2</span>, self.n_split // <span class="number">2</span>, c // self.n_split, t)</span><br><span class="line">z = z.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>).contiguous().view(b, c, t) * x_mask</span><br></pre></td></tr></table></figure>
<p>根据<a target="_blank" rel="noopener" href="https://www.notion.so/Flow-d0415a7ad07040f19c3ff7a2c3023e35?pvs=21">Glow中1x1卷积中weight</a>的要求，要求weight是正交矩阵，其对应的代码为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.qr将一个矩阵分解为一个正交矩阵与一个上三角矩阵的乘积，其中的第一个部分是正交矩阵。</span></span><br><span class="line">w_init = torch.qr(torch.FloatTensor(self.n_split, self.n_split).normal_())[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">if</span> torch.det(w_init) &lt; <span class="number">0</span>:</span><br><span class="line">  w_init[:,<span class="number">0</span>] = -<span class="number">1</span> * w_init[:,<span class="number">0</span>]</span><br><span class="line">self.weight = nn.Parameter(w_init)</span><br></pre></td></tr></table></figure>
<p><strong>Encoder and Duration Predictor</strong></p>
<p>encoder的结构与transformer TTS相同，为了得到先验分布的统计量 $\mu$ 和 $\sigma$ ，在encoder的尾部添加了一个线性映射层。dutation preictor是由两个卷积层、ReLU激活函数接一个线性映射层组成，和FastSpeech一致。</p>
<h2 id="实验">实验</h2>
<p>为了验证方法的有效性，在两个数据集上进行验证：单说话人和多说话人。单说话人使用LJSpeech数据集，其包含13100条语音，（12500条用来训练，100条用于验证，500条用于测试）。多说话人上使用LibriTTS数据集的train-clean-100，包含247个说话人，总共54小时。数据处理过程中会对所有音频开头和结尾的静音段去掉，并且会去掉文本长度超过190的音频（29181条用来训练、88用来验证、442用来测试）。此外，集外测试集使用227条来自Harry Potter and the Philosopher’s Stone的文本进行，最大长度超过800。</p>
<p>base模型选择Tacotron2，vocoder使用WaveGlow，建模单元为音素(phoneme)。</p>
<p>训练的过程中我们设置先验分布的标准差 $\sigma$ 为常量1，Adam作为optimizer，Noam作为learning rate scheduler，使用两张V100混合精度训练3天，迭代240K步。</p>
<p>为了训练多说话人的 Glow-TTS 模型，speaker embedding 被用在 flow-based 的 decoder 以及 text encoder上，对应的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoder为text encoder、decoder为flow-based decoder，x为文本输入，y为梅尔谱输入，g为说话人embedding</span></span><br><span class="line">x_m, x_logs, logw, x_mask = self.encoder(x, x_lengths, g=g)</span><br><span class="line">z, logdet = self.decoder(y, z_mask, g=g, reverse=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="结果">结果</h2>
<h3 id="音频质量">音频质量</h3>
<p>音频质量使用 MOS (mean opinion score) 作为评价指标，随机从测试集得到 50 段音频然后使用 Amazon Mechanical Turk 工具获得 MOS 值，结果如下：</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS4.png" width="50%"/>
</div>
<p>测试时会变换不同的 standard deviation (i.e., temperature T)分别得到 MOS值，结果表明当temperature 设置为 0.333 时 MOS值最高，均好于Tacotron 2的表现。</p>
<h3 id="采样速度和鲁棒性">采样速度和鲁棒性</h3>
<div  style="text-align: center;">
<img src="../images/GlowTTS5.png" width="70%"/>
</div>
<p><strong>采样速度 (Sampling Speed)</strong>：如上图所示，因为 Glow-TTS 是非自回归模型，所以其生成 mel-spectrogram 的时间与其文本的长度无关 (40ms)，而 Tacotron 2模型与所要生成的文本长度线性相关。使用 Glow-TTS生成 1 分钟的音频所需的时间为 1.5s，其中vocoder占用了 95% 的时间，而 Glow-TTS 只占用了其中的 5% (55ms), 几乎是可以忽略不计的。</p>
<p><strong>鲁棒性 (Robustness)</strong>：Glow-TTS 对长音频的合成具有很强的鲁棒性，如上图(b)，当使用Harry Potter and the Philosopher’s Stone中的长文本作为测试集时，Tacotron 2合成的音频的 CER 随文本长度的升高显著上升，而 Glow-TTS 几乎保持不变。</p>
<h3 id="多样性和可控性">多样性和可控性</h3>
<p>通过 $f_{dec}(z)$ 可以将采样出的潜在表示 $z$ 合成梅尔谱，而 $z \sim \mathcal{N}(\mu, T)$ 是通过如下公式得到：</p>
<p>$$<br>
z = \mu + \epsilon * T<br>
$$</p>
<p>其中 $\epsilon$ 表示从标准正态分布进行采样的一个样本， $\mu$ 和 $T$ 是先验分布的均值和标准差。以下从几个方面说明 Glow-TTS 的可控性：</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS6.png" width="70%"/>
</div>
<ul>
<li><strong>重音和语调 (strss and intonation)</strong>: 如图(a)，通过改变 $\epsilon$ 可以控制重音和语调；</li>
<li><strong>音高 (pitch):</strong> 如图(b)，可以通过改变 $T$ 进行控制；</li>
<li><strong>语速 (speaking rate)</strong>：可以让duration preditor 乘上不同的值进行控制；</li>
</ul>
<h3 id="Multi-Speaker-TTS">Multi-Speaker TTS</h3>
<p>将从 Audio Quality、Speaker-Dependent Duration、Voice Conversion几个方向进行评估。</p>
<p><strong>音频质量</strong>：从 50 个说话人中挑选出 50 段音频得到 MOS 值，结果如下，可见Glow-TTS具有更好的音频质量。</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS7.png" width="50%"/>
</div>
<p><strong>Speaker-Dependent Duration</strong>：不同说话人对相同token预测的时长如下图，可见对不同说话人相同token所预测的时长具有很大的区分性，说明提出的模型具有Speaker-Dependent Duration。</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS8.png" width="50%"/>
</div>
<p><strong>声音转换</strong>：因为 Glow-TTS 会对潜在表征 $z$ 和说话人进行解耦（因为在训练的时候没有提供说话人信息给 encoder ），所以可以用来进行声音转换，为了测试这种解耦的程度，本文把 Ground Truth 梅尔谱 $x$ 和说话人信息 $s$ 转换成潜在表示 $z$ ，即 $z=f^{-1}_{dec}(x|s)$ ，然后提供另外一个说话人的信息用于合成新的梅尔谱，即 $\hat{x}=f_{dec}(z|\hat{s})$，然后分别得到 $x$ 和 $\hat{x}$ 的 pitch 图，如下图所示，不同说话人的 pitch 有很大的区分性，但趋势是相同的，说明该模型能完成潜在表征 $z$ 和说话人信息的解耦。</p>
<div  style="text-align: center;">
<img src="../images/GlowTTS9.png" width="50%"/>
</div>
<h2 id="疑问">疑问</h2>
<ol>
<li>stop gradient的作用是什么？</li>
</ol>
<blockquote>
<p>训练时可以通过单调对齐搜索得到spectrogram和text的对齐关系，并可以通过不断迭代获取更好的对齐，完全可以不用Duration Predictor的参与。但是在推理时并没有可以参考的spectrogram，所以训练时“顺带”训了一个Duration Predictor模块以便推理时可以得到对齐关系，以便从text表示中生成spectrogram。</p>
</blockquote>
<h2 id="参考">参考</h2>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/jaywalnut310/glow-tts">https://github.com/jaywalnut310/glow-tts</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11129">https://arxiv.org/abs/2005.11129</a></li>
<li><a href="https://loganliu66.github.io/FlowModel/">https://loganliu66.github.io/FlowModel/</a></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/TTS/" rel="tag"># TTS</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/FlowModel/" rel="prev" title="Flow Model">
      <i class="fa fa-chevron-left"></i> Flow Model
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">2.</span> <span class="nav-text">前置知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="nav-number">3.1.</span> <span class="nav-text">训练与推理过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E8%B0%83%E5%AF%B9%E9%BD%90%E6%90%9C%E7%B4%A2"><span class="nav-number">3.2.</span> <span class="nav-text">单调对齐搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">3.3.</span> <span class="nav-text">模型架构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">4.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C"><span class="nav-number">5.</span> <span class="nav-text">结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9F%B3%E9%A2%91%E8%B4%A8%E9%87%8F"><span class="nav-number">5.1.</span> <span class="nav-text">音频质量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7%E9%80%9F%E5%BA%A6%E5%92%8C%E9%B2%81%E6%A3%92%E6%80%A7"><span class="nav-number">5.2.</span> <span class="nav-text">采样速度和鲁棒性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E6%A0%B7%E6%80%A7%E5%92%8C%E5%8F%AF%E6%8E%A7%E6%80%A7"><span class="nav-number">5.3.</span> <span class="nav-text">多样性和可控性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Speaker-TTS"><span class="nav-number">5.4.</span> <span class="nav-text">Multi-Speaker TTS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%96%91%E9%97%AE"><span class="nav-number">6.</span> <span class="nav-text">疑问</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Logan Liu"
      src="/images/favicon-128x128.png">
  <p class="site-author-name" itemprop="name">Logan Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Logan Liu</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
